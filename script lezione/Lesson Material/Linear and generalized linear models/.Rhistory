start=c(70,1)
d=list(int.lo=c(-Inf,seq(66,74,by=2)),
int.hi=c(seq(66,74,by=2), Inf),
f=c(14,30,49,70,33,15))
library(coda)
start=c(70,1)
fit=laplace(groupeddatapost,start,d)
fit
?laplace
start=c(65,1)
proposal=list(var=fit$var,scale=0.2)
bayesfit=rwmetrop(groupeddatapost,proposal,start,10000,d)
bayesfit$accept
apply(bayesfit$par,2,mean)
apply(bayesfit$par,2,sd)
dimnames(bayesfit$par)[[2]]=c("mu","log sigma")
xyplot(mcmc(bayesfit$par),col="black")
mcmc(bayesfit$par[1:1000,])
library(LearnBayes)
d=list(int.lo=c(-Inf,seq(66,74,by=2)),
int.hi=c(seq(66,74,by=2), Inf),
f=c(14,30,49,70,33,15))
library(coda)
start=c(70,1)
fit=laplace(groupeddatapost,start,d)
# Let us choose an UNLUCKY initial point for the chain, and a small scale factor for the proposal c=0.2;
start=c(65,1)
proposal=list(var=fit$var,scale=0.2)
bayesfit=rwmetrop(groupeddatapost,proposal,start,10000,d)
bayesfit$accept
apply(bayesfit$par,2,mean)
apply(bayesfit$par,2,sd)
#Assegno i nomi alle colonne di bayesfit$par
dimnames(bayesfit$par)[[2]]=c("mu","log sigma")
xyplot(mcmc(bayesfit$par),col="black")
x11()
library(LearnBayes)
d=list(int.lo=c(-Inf,seq(66,74,by=2)),
int.hi=c(seq(66,74,by=2), Inf),
f=c(14,30,49,70,33,15))
library(coda)
start=c(70,1)
fit=laplace(groupeddatapost,start,d)
# Let us choose an UNLUCKY initial point for the chain, and a small scale factor for the proposal c=0.2;
start=c(65,1)
proposal=list(var=fit$var,scale=0.2)
bayesfit=rwmetrop(groupeddatapost,proposal,start,10000,d)
bayesfit$accept
apply(bayesfit$par,2,mean)
apply(bayesfit$par,2,sd)
#Assegno i nomi alle colonne di bayesfit$par
dimnames(bayesfit$par)[[2]]=c("mu","log sigma")
xyplot(mcmc(bayesfit$par),col="black")
x11()
xyplot.mcmc(mcmc(bayesfit$par),col="black")
?xyplot
library(lattice)
xyplot.mcmc(mcmc(bayesfit$par),col="black")
xyplot(mcmc(bayesfit$par),col="black")
xyplot(mcmc(bayesfit$par),col="black")
xyplot(mcmc(bayesfit$par[1:1000,]),col="black")
dimnames(bayesfit$par)[[2]]=c("mu","log sigma")
xyplot(mcmc(bayesfit$par),col="black")
x11()
# BURN-IN: about 600 iterations
xyplot(mcmc(bayesfit$par[1:1000,]),col="black")
# If we forget the first 2000 iterations
xyplot(mcmc(bayesfit$par[-c(1:2000),]),col="black")
xyplot(mcmc(bayesfit$par[1:1000,]),col="black")
xyplot(mcmc(bayesfit$par[-c(1:2000),]),col="black")
par(mfrow=c(2,1))
# Plots of AUTOCORRELATION between X_n and X_{n+k} by CODA
autocorr.plot(mcmc(bayesfit$par[-c(1:2000),]),auto.layout=FALSE)
summary(mcmc(bayesfit$par[-c(1:2000),]))
summary(mcmc(bayesfit$par[-c(1:2000),]))
batchSE(mcmc(bayesfit$par[-c(1:2000),]), batchSize=50)
effectiveSize(mcmc(bayesfit$par[-c(1:2000),])) # su 8000 iterazioni
start=c(70,1)
proposal=list(var=fit$var,scale=2.0)
bayesfit2=rwmetrop(groupeddatapost,proposal,start,10000,d)
bayesfit2$accept
apply(bayesfit2$par,2,mean)
apply(bayesfit2$par,2,sd)
dimnames(bayesfit2$par)[[2]]=c("mu","log sigma")
xyplot(mcmc(bayesfit2$par),col="black")
x11()
xyplot(mcmc(bayesfit2$par[1:30,]),col="black")
sim.parameters=mcmc(bayesfit2$par[-c(1:2000),])
xyplot(mcmc(bayesfit2$par[-c(1:2000),]),col="black")
par(mfrow=c(2,1))
autocorr.plot(sim.parameters,auto.layout=FALSE)
summary(sim.parameters)
batchSE(sim.parameters, batchSize=50)
xyplot(mcmc(bayesfit2$par[1:30,]),col="black")
sim.parameters=mcmc(bayesfit2$par[-c(1:2000),])
xyplot(mcmc(bayesfit2$par[-c(1:2000),]),col="black")
par(mfrow=c(2,1))
autocorr.plot(sim.parameters,auto.layout=FALSE)
summary(sim.parameters)
batchSE(sim.parameters, batchSize=50)
densplot(mcmc(bayesfit2$par[-c(1:2000),1]))
densplot(mcmc(bayesfit2$par[-c(1:2000),2]),ylim=c(0,8))
effectiveSize(mcmc(bayesfit2$par[-c(1:2000),]))
mu0<-1.9  ; t20<-0.95^2
s20<-.01 ; nu0<-1
1/s20
mu0<-1.9  ; t20<-0.95^2
s20<-.01 ; nu0<-1
1/s20
y<-c(1.64,1.70,1.72,1.74,1.82,1.82,1.82,1.90,2.08)
n<-length(y) ; mean.y<-mean(y) ; var.y<-var(y)
set.seed(1)
S<-1000
PHI<-matrix(nrow=S,ncol=2) #Inizializzo una matrice con 2 colonne e S righe
PHI[1,]<-phi<-c( mean.y, 1/var.y) # Initial point of the chain
for(s in 2:S) {
mun<-  ( mu0/t20 + n*mean.y*phi[2] ) / ( 1/t20 + n*phi[2] ) #updated mean
t2n<- 1/( 1/t20 + n*phi[2] ) #updated variance
phi[1]<-rnorm(1, mun, sqrt(t2n) )
nun<- nu0+n             # it does NOT change with the iteration
s2n<- (nu0*s20 + (n-1)*var.y + n*(mean.y-phi[1])^2 ) /nun #updated parameter
phi[2]<- rgamma(1, nun/2, nun*s2n/2)
PHI[s,]<-phi         }
#################################################
########### Chapter 6 - Hoff's book #############
#################################################
# The data are wing lenght (in mm) of n=9 midges (moscerini)
# The model : Y_i are conditionally iid N(theta,sigma^2)
# Both theta and tau=1/sigma^2 are unknown
# Prior: pi(theta,tau)= pi(theta)* pi(tau)
#        theta ~ N(mu0, t20) (mu0 is the mean, t20 is the variance)
#        tau ~ gamma(nu0/2, (nu0*s20)/2 )
mu0<-1.9  ; t20<-0.95^2
s20<-.01 ; nu0<-1
1/s20
#data
y<-c(1.64,1.70,1.72,1.74,1.82,1.82,1.82,1.90,2.08)
n<-length(y) ; mean.y<-mean(y) ; var.y<-var(y)
### Generate a Gibbs sampler MCMC of S draws
set.seed(1)
S<-1000
PHI<-matrix(nrow=S,ncol=2) #Inizializzo una matrice con 2 colonne e S righe
# it contains simulated values of the BIVARIATE Gibbs sampler MC
PHI[1,]<-phi<-c( mean.y, 1/var.y) # Initial point of the chain
################################
####### Gibbs sampling #########
for(s in 2:S) {
# the bidim. vector phi contains the "current" value of the chain
# generate a new theta value from its full conditional
mun<-  ( mu0/t20 + n*mean.y*phi[2] ) / ( 1/t20 + n*phi[2] ) #updated mean
t2n<- 1/( 1/t20 + n*phi[2] ) #updated variance
phi[1]<-rnorm(1, mun, sqrt(t2n) )
# generate a new tau=1/sigma^2 value from its full conditional
nun<- nu0+n             # it does NOT change with the iteration
s2n<- (nu0*s20 + (n-1)*var.y + n*(mean.y-phi[1])^2 ) /nun #updated parameter
phi[2]<- rgamma(1, nun/2, nun*s2n/2)
PHI[s,]<-phi         }
###
PHI
par(mfrow=c(1,3),mar=c(2.75,2.75,.5,.5),mgp=c(1.70,.70,0))
m1<-1
plot( PHI[1:m1,],type="p",xlim=range(PHI[1:100,1]), ylim=range(PHI[1:100,2]),
lty=1,col="gray",xlab=expression(theta),ylab=expression(tau^2))
text(  PHI[1:m1,1], PHI[1:m1,2], c(1:m1) )
m1<-2
plot( PHI[1:m1,],type="l",xlim=range(PHI[1:100,1]), ylim=range(PHI[1:100,2]),
lty=1,col="gray",xlab=expression(theta),ylab=expression(tau^2))
text(  PHI[1:m1,1], PHI[1:m1,2], c(1:m1) )
m1<-3
plot( PHI[1:m1,],type="l",xlim=range(PHI[1:100,1]), ylim=range(PHI[1:100,2]),
lty=1,col="gray",xlab=expression(theta),ylab=expression(tau^2))
text(  PHI[1:m1,1], PHI[1:m1,2], c(1:m1) )
x11()
#pdf("fig6_2.pdf",height=1.75,width=5,family="Times")
par(mfrow=c(1,3),mar=c(2.75,2.75,.5,.5),mgp=c(1.70,.70,0))
m1<-5
plot( PHI[1:m1,],type="l",xlim=range(PHI[1:100,1]), ylim=range(PHI[1:100,2]),
lty=1,col="gray",xlab=expression(theta),ylab=expression(tau^2))
text(  PHI[1:m1,1], PHI[1:m1,2], c(1:m1) )
m1<-15
plot( PHI[1:m1,],type="l",xlim=range(PHI[1:100,1]), ylim=range(PHI[1:100,2]),
lty=1,col="gray",xlab=expression(theta),ylab=expression(tau^2))
text(  PHI[1:m1,1], PHI[1:m1,2], c(1:m1) )
m1<-100
plot( PHI[1:m1,],type="l",xlim=range(PHI[1:100,1]), ylim=range(PHI[1:100,2]),
lty=1,col="gray",xlab=expression(theta),ylab=expression(tau^2))
text(  PHI[1:m1,1], PHI[1:m1,2], c(1:m1) )
dev.off()
sseq<-1:1000
plot(PHI[sseq,1],PHI[sseq,2],xlab=expression(theta), ylab=expression(tau^2) ,
xlim=range(PHI[,1]),ylim=range(PHI[,2]))
plot(PHI[sseq,1],PHI[sseq,2],xlab=expression(theta), ylab=expression(tau^2) ,
xlim=range(PHI[,1]),ylim=range(PHI[,2]))
par(mfrow=c(1,2),mar=c(2.75,2.75,.5,.5),mgp=c(1.70,.70,0))
sseq<-1:1000
plot(density(PHI[,1],adj=2),  xlab=expression(theta),main="",
xlim=c(1.55,2.05),
ylab=expression( paste(italic("p("),
theta,"|",italic(y[1]),"...",italic(y[n]),")",sep="")))
abline(v=quantile(PHI[,1],prob=c(.025,.975)),lwd=2,col="gray")
plot(density(PHI[,2],adj=2), xlab=expression(tau^2),main="",
ylab=expression( paste(italic("p("),
tau^2,"|",italic(y[1]),"...",italic(y[n]),")",sep="")))
abline(v=quantile(PHI[,2],prob=c(.025,.975)),lwd=2,col="gray")
dev.off()
quantile(PHI[,1],c(.025,.5,.975)) # Posterior CI of theta
quantile(PHI[,2],c(.025,.5, .975)) # Posterior CI of tau
quantile(1/sqrt(PHI[,2]),c(.025,.5, .975))  # Posterior CI of sigma
par(mfrow=c(1,2))
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
plot(PHI[,1],xlab="iteration",ylab=expression(theta))
plot(1/PHI[,2],xlab="iteration",ylab=expression(sigma^2))
dev.off()
par(mfrow=c(1,2))
plot(ts(PHI[,1]),xlab="iteration",ylab=expression(theta))
plot(ts(1/PHI[,2]),xlab="iteration",ylab=expression(sigma^2))
library(coda)
par(mfrow=c(1,2))
acf(PHI[,1]) -> tmp1
acf(1/PHI[,2]) -> tmp2
effectiveSize( PHI[,1] )
effectiveSize(1/PHI[,2] )
dev.off()
library(LearnBayes)
data(birdextinct)
attach(birdextinct)
plot(birdextinct)
logtime=log(time)
plot(nesting,logtime)
out = (logtime > 3)
text(nesting[out], logtime[out], label=species[out], pos = 2)
x11()
# Command JITTER adds a small amount of noise to a numeric vector
par(mfrow=c(1,2))
plot(jitter(size),logtime,xaxp=c(0,1,1))
plot(jitter(status),logtime,xaxp=c(0,1,1))
x11()
# Command JITTER adds a small amount of noise to a numeric vector
par(mfrow=c(1,2))
plot(jitter(size),logtime,xaxp=c(0,1,1))
plot(jitter(status),logtime,xaxp=c(0,1,1))
fit=lm(logtime~nesting+size+status,data=birdextinct,x=TRUE,y=TRUE)
summary(fit)
fit$x
fit$y
plot(fit$residual)
theta.sample=blinreg(fit$y,fit$x,2)
theta.sample
theta.sample=blinreg(fit$y,fit$x,2)
theta.sample
theta.sample=blinreg(fit$y,fit$x,5000)
par(mfrow=c(2,2))
hist(theta.sample$beta[,1],main="INTERCEPT",xlab=expression(beta[0]),prob=T)
hist(theta.sample$beta[,2],main="NESTING",xlab=expression(beta[1]),prob=T)
hist(theta.sample$beta[,3],main="SIZE",xlab=expression(beta[2]),prob=T)
hist(theta.sample$beta[,4],main="STATUS",xlab=expression(beta[3]),prob=T)
X11()
par(mfrow=c(1,2))
#La distr. FINALE di tau=1/sigma^2 ? gamma((n-p)/2, ((n-p)/2)*S^2)
#Ecco l'istogramma:
hist(1/((theta.sample$sigma)^2),main="PRECISIONE TAU",xlab=expression(tau),prob=T)
hist(theta.sample$sigma,main="ERROR SD",xlab=expression(sigma),prob=T)
apply(theta.sample$beta,2,quantile,c(.05,.5,.95))
apply(theta.sample$beta,2,mean)
fit$coefficients
quantile(theta.sample$sigma,c(.05,.5,.95))
mean(theta.sample$sigma)
sqrt(sum(fit$residuals^2)/(62-4))
cov1=c(1,4,0,0) #A
cov2=c(1,4,1,0) #B
cov3=c(1,4,0,1) #C
cov4=c(1,4,1,1) #D
X1=rbind(cov1,cov2,cov3,cov4)
mean.draws=blinregexpected(X1,theta.sample)
mean.draws
c.labels=c("A","B","C","D")
par(mfrow=c(2,2))
for (j in 1:4)
hist(mean.draws[,j], main=paste("Covariate set",c.labels[j]),xlab="log TIME",prob=T)
pred.draws=blinregpred(X1,theta.sample)
c.labels=c("A","B","C","D")
par(mfrow=c(2,2))
for (j in 1:4)
hist(pred.draws[,j],main=paste("Covariate set",c.labels[j]),xlab="log TIME",prob=T)
prior=list(b0=c(0,0,0,0), c0=10)  #c0\in [10,100]
prior
theta2.sample=blinreg(fit$y,fit$x,5000,prior=prior)
x11()
par(mfrow=c(2,2))
hist(theta2.sample$beta[,1],main="INTERCEPT",xlab=expression(beta[0]),prob=T)
hist(theta2.sample$beta[,2],main="NESTING",xlab=expression(beta[1]),prob=T)
hist(theta2.sample$beta[,3],main="SIZE",xlab=expression(beta[2]),prob=T)
hist(theta2.sample$beta[,4],main="STATUS",xlab=expression(beta[3]),prob=T)
X11()
par(mfrow=c(1,2))
hist(1/((theta2.sample$sigma)^2),main="PRECISIONE TAU",xlab=expression(tau),prob=T)
hist(theta2.sample$sigma,main="ERROR SD",xlab=expression(sigma),prob=T)
apply(theta2.sample$beta,2,quantile,c(.05,.5,.95))
apply(theta2.sample$beta,2,mean)
fit$coefficients # stime frequentiste
quantile(theta2.sample$sigma,c(.05,.5,.95))
#stima bayesiana di sigma^2 - media a posteriori - ? maggiore della stima ai minimi quadrati
mean(theta2.sample$sigma)
#stima frequentista ai mimini quadrati di sigma
sqrt(sum(fit$residuals^2)/(62-4))
cov1=c(1,4,0,0) #A
cov2=c(1,4,1,0) #B
cov3=c(1,4,0,1) #C
cov4=c(1,4,1,1) #D
X1=rbind(cov1,cov2,cov3,cov4)
# La funzione blinregexpected prende in INPUT la nuova matrice disegno
# e il campione generato dalla posterior e ne fa la combinazione lineare
mean.draws=blinregexpected(X1,theta2.sample)
c.labels=c("A","B","C","D")
par(mfrow=c(2,2))
for (j in 1:4)
hist(mean.draws[,j], main=paste("Covariate set",c.labels[j]),xlab="log TIME",prob=T)
cov1=c(1,4,0,0)
cov2=c(1,4,1,0)
cov3=c(1,4,0,1)
cov4=c(1,4,1,1)
X1=rbind(cov1,cov2,cov3,cov4)
#La funzione blinregpred genera un valore dalle verosimiglianze valutate
pred.draws=blinregpred(X1,theta2.sample)
c.labels=c("A","B","C","D")
par(mfrow=c(2,2))
for (j in 1:4)
hist(pred.draws[,j],main=paste("Covariate set",c.labels[j]),xlab="log TIME",prob=T)
pred.draws=blinregpred(fit$x,theta2.sample)
pred.sum=apply(pred.draws,2,quantile,c(.05,.95))
par(mfrow=c(1,1))
ind=1:length(logtime)
matplot(rbind(ind,ind),pred.sum,type="l",lty=1,col=1,xlab="INDEX",ylab="log TIME")
points(ind,logtime,pch=19)
dim (pred.draws)
dim (pred.sum)
out=(logtime>pred.sum[2,]| logtime<pred.sum[1,])
text(ind[out], logtime[out], label=species[out], pos = 4)
ind_pvalue=t((t(pred.draws)>logtime))
pred.suptail=apply(ind_pvalue,2,sum)/5000
prob.tail<-rep(0, 62)
for (i in 1 : 62)
prob.tail[i]=min(pred.suptail[i],1-pred.suptail[i])
## this is the BAYESIAN PREDICTIVE p-value of item i
x11()
par(mfrow=c(1,2))
plot(ind, prob.tail)
abline(h=0.05)
plot(ind,logtime,pch=19)
out2=(prob.tail<0.05)
text(ind[out2], logtime[out2], label=species[out2], pos = 4)
setwd("~/Stat.-Bayesiana/script lezione/Lesson Material/Linear and generalized linear models")
Y=read.table("school.mathscore.txt")
Y
dim(Y)
head(Y)
m = length(unique(Y[,1]))
n<-sv<-ybar<-rep(NA,m)
for(j in 1:m)
{
ybar[j]<- mean(Y[Y[,1]==j,2])   #medie per gruppo
sv[j]<-var(Y[Y[,1]==j,2])  #varianze empiriche in ogni gruppo
n[j]<-sum(Y[,1]==j)        #numerosit? in ogni gruppo (scuola)
}
ybar
unique(Y[,1])
index<-unique(Y[,1])
for(j in index)
{
ybar[j]<- mean(Y[Y[,1]==j,2])   #medie per gruppo
sv[j]<-var(Y[Y[,1]==j,2])  #varianze empiriche in ogni gruppo
n[j]<-sum(Y[,1]==j)        #numerosit? in ogni gruppo (scuola)
}
ybar
index<-unique(Y[,1])
for(j in index)
j
Y[,1]==j
Y[Y[,1]==j,2]
Y[Y[,1]==j,2])
n<-sv<-ybar<-c()
for(j in index)
{
ybar<- c(ybar,mean(Y[Y[,1]==j,2]))  #medie per gruppo
sv<-c(sv,var(Y[Y[,1]==j,2]))  #varianze empiriche in ogni gruppo
n<-c(n,sum(Y[,1]==j))   #numerosit? in ogni gruppo (scuola)
}
summary(ybar)
n<-sv<-ybar<-c()
for(j in index)
{
ybar<- c(ybar,mean(Y[Y[,1]==j,2]))  #medie per gruppo
sv<-c(sv,var(Y[Y[,1]==j,2]))  #varianze empiriche in ogni gruppo
n<-c(n,sum(Y[,1]==j))   #numerosit? in ogni gruppo (scuola)
}
ybar
n
sv
Y[Y[,1]==j,2]
index
par(mfrow=c(1,2),mar=c(3,3,1,1),mgp=c(1.75,.75,0))
hist(ybar,main="",xlab="sample mean")
plot(n,ybar,xlab="sample size",ylab="sample mean")
x11()
##### IPERPARAMETRI assegnati sulla base di INFORMAZIONI a priori, cio? a prescindere
#####   dai dati di questo specifico esperimento
par(mfrow=c(1,1))
nu0<-1  ; s20<-100   # iperparametri per la prior di sigma^2 = within-group variance
pippo=rgamma(1000,shape=nu0/2, rate=nu0/2*s20 )
hist(pippo,prob=T,main='Prior per 1/sigma^2')
mean(1/pippo);var(1/pippo)  # media e varianza di sigma^2 sono +infinito!
eta0<-1 ; t20<-100 # iperparametri per la prior di tau^2
mu0<-50 ; g20<-25  # IMPORTANT: iperparametri di mu
x11()
##### IPERPARAMETRI assegnati sulla base di INFORMAZIONI a priori, cio? a prescindere
#####   dai dati di questo specifico esperimento
par(mfrow=c(1,1))
nu0<-1  ; s20<-100   # iperparametri per la prior di sigma^2 = within-group variance
pippo=rgamma(1000,shape=nu0/2, rate=nu0/2*s20 )
hist(pippo,prob=T,main='Prior per 1/sigma^2')
mean(1/pippo);var(1/pippo)  # media e varianza di sigma^2 sono +infinito!
eta0<-1 ; t20<-100 # iperparametri per la prior di tau^2
mu0<-50 ; g20<-25  # IMPORTANT: iperparametri di mu
theta<-ybar
sigma2<-mean(sv)
mu<-mean(theta)
tau2<-var(theta)
m = length(unique(Y[,1]))
n<-sv<-ybar<-rep(NA,m)
for(j in 1:m)
{
ybar[j]<- mean(Y[Y[,1]==j,2])   #medie per gruppo
sv[j]<-var(Y[Y[,1]==j,2])  #varianze empiriche in ogni gruppo
n[j]<-sum(Y[,1]==j)        #numerosit? in ogni gruppo (scuola)
}
m = length(unique(Y[,1]))
n<-sv<-ybar<-rep(NA,m)
for(j in 1:m)
{
ybar[j]<- mean(Y[Y[,1]==unique(Y[,1])[j],2])   #medie per gruppo
sv[j]<-var(Y[Y[,1]==unique(Y[,1])[j],2])  #varianze empiriche in ogni gruppo
n[j]<-sum(Y[,1]==unique(Y[,1]))        #numerosit? in ogni gruppo (scuola)
}
Y[Y[,1]==unique(Y[,1])[j],2]
Y=read.table("school.mathscore.txt")
Y
head(Y)
dim(Y)
Y<-Y[,c(2,4)]
dim(Y)
## Scores at the math test for 1993 students in 100 US schools
## First column: index of the school of the student
## Second column: math score of the student
### group means of the math scores
m = length(unique(Y[,1]))
n<-sv<-ybar<-rep(NA,m)
for(j in 1:m)
{
ybar[j]<- mean(Y[Y[,1]==unique(Y[,1])[j],2])   #medie per gruppo
sv[j]<-var(Y[Y[,1]==unique(Y[,1])[j],2])  #varianze empiriche in ogni gruppo
n[j]<-sum(Y[,1]==unique(Y[,1]))        #numerosit? in ogni gruppo (scuola)
}
ybar
n
sv
par(mfrow=c(1,2),mar=c(3,3,1,1),mgp=c(1.75,.75,0))
hist(ybar,main="",xlab="sample mean")
plot(n,ybar,xlab="sample size",ylab="sample mean")
Y=read.table("school.mathscore.txt")
m = length(unique(Y[,1]))
n<-sv<-ybar<-rep(NA,m)
for(j in 1:m)
{
ybar[j]<- mean(Y[Y[,1]==unique(Y[,1])[j],2])   #medie per gruppo
sv[j]<-var(Y[Y[,1]==unique(Y[,1])[j],2])  #varianze empiriche in ogni gruppo
n[j]<-sum(Y[,1]==unique(Y[,1]))        #numerosit? in ogni gruppo (scuola)
}
#summary(ybar)
ybar
n
sv
x11()
x11()
##### IPERPARAMETRI assegnati sulla base di INFORMAZIONI a priori, cio? a prescindere
#####   dai dati di questo specifico esperimento
par(mfrow=c(1,1))
nu0<-1  ; s20<-100   # iperparametri per la prior di sigma^2 = within-group variance
pippo=rgamma(1000,shape=nu0/2, rate=nu0/2*s20 )
hist(pippo,prob=T,main='Prior per 1/sigma^2')
mean(1/pippo);var(1/pippo)  # media e varianza di sigma^2 sono +infinito!
eta0<-1 ; t20<-100 # iperparametri per la prior di tau^2
mu0<-50 ; g20<-25  # IMPORTANT: iperparametri di mu
#### A priori P(\mu \in (40,60))= 0.94 circa e E(\mu)=50 che ? la media nazionale
#### Prior informativa, ricavata da info sul test somministato a tutta la nazione
dev.off()
theta<-ybar
sigma2<-mean(sv)
mu<-mean(theta)
tau2<-var(theta)
###
### setup MCMC
set.seed(1)
S<-3000
#S<-5
THETA<-matrix( nrow=S,ncol=m)
MST<-matrix( nrow=S,ncol=3)
MST
plot(1:100,theta.hat, cex=0.5,main="90% credible intervals",xlim=c(1,100),ylim=c(35,70),xlab='school',ylab='')
for (i in 1:100) {
probint=quantile(THETA[,i], c(0.05, 0.95))
lines(i * c(1, 1), probint)
}
